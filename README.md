# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y). (https://archive.ics.uci.edu/ml/datasets/bank+marketing).

The best performing model was a VotingEnsemble Algorithm generated by AutoML with an accuracy score of 0.9160.
Generated by HyperDrive, the Logistic Regression Algorithm performed equally well with an accuracy score of 0.9126.

## Scikit-learn Pipeline
HyperDrive Pipeline:
1. Start Workspace
2. Create/Use compute instance
3. Create HyperDriveConfig\
3.1 Specify parameter sampler\
3.2 Specify policy for early stopping\
3.3 Create estimator
4. Run HyperDrive experiment
5. Select best hyperparameters/run/model
6. Save best model
7. Register best model

Classification Algorithm:\
LogisticRegression (sklearn.linear_model)\
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html 

Hyperparameter Tuning via HyperDrive:\
3. Primary metric to optimse: Accuracy\
3.1 Parameter space / hyperparameters to optimise: regularization strength between 0 and 1 and max iterations of 10, 50 or 100

- What are the benefits of the chosen parameter sampler?\
RandomParameterSampling is optimised for speed because it picks randomly hyperparameter values instead of going though every single value, i.e., it allows to achieve an optimal primary metric for a relatively short period of time.

- What are the benefits of the chosen early stopping policy?\
Bandit Policy terminates runs which fall outside of the top n% range every k interval, saving time of the experiment. In this case, any run whose best metric is less than (1/(1+0.1) or 91% of the best performing run will be terminated.

Final model metrics:\
Accuracy: 0.9126001456664239\
Max iterations: 100\
Regularization Strength: 0.11213843976728655

## AutoML
VotingEnsemble Algorithm:\
Ensembled_algorithms : ['XGBoostClassifier', 'XGBoostClassifier', 'LightGBM', 'XGBoostClassifier', 'LogisticRegression', 'RandomForest', 'RandomForest']\
Ensemble_weights : [0.14285714285714285, 0.07142857142857142, 0.2857142857142857, 0.21428571428571427, 0.07142857142857142, 0.07142857142857142, 0.14285714285714285]

Final Model Metric:\
Accuracy: 0.91603

## Pipeline comparison
HyperDrive and AutoML performed equally well in terms of accuracy. Unlike AutoML, which generates ensemble models (combination of multiple models), HyperDrive sticks with one algorithm only (logistic regression). Whereas HyperDrive tunes hyperparamters only, AutoML selects different estimators, engineers different features and chooses different hypterparameters all at once.

## Future work
Imbalanced data handeling: Data guardrails are run by AutoML when automatic featurization is enabled. This is a sequence of checks over the input data to ensure high quality data is being used to train models. In this case, imbalanced classes were detected in the dataset by AutoML, which could lead to a falsely perceived positive effect of a model's accuracy because the input data has bias towards one class.